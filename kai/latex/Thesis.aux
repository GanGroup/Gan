\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{10.48550/arxiv.1704.00028}
\citation{10.1002/mgea.30}
\citation{10.1016/j.media.2019.101552}
\citation{10.1016/j.artmed.2020.101938}
\citation{10.1016/j.neucom.2018.09.013}
\citation{10.1109/iccv.2019.00453}
\citation{10.1016/j.media.2019.101552}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{10.48550/arxiv.1711.00658}
\citation{10.48550/arxiv.2110.11271}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Historical Models of Image Generation}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Noise Contrastive Estimation}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Principles of NCE}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}NCE Architecture and Process}{8}{subsubsection.2.1.1.1}\protected@file@percent }
\citation{10.48550/arxiv.1711.00658}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces NCE architecture showing the input, hidden, and output layers.\relax }}{9}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NCE_structure}{{2.1}{9}{NCE architecture showing the input, hidden, and output layers.\relax }{figure.caption.6}{}}
\citation{10.18653/v1/e17-2003}
\citation{10.18653/v1/e17-2003}
\citation{10.21437/interspeech.2016-1295}
\citation{10.48550/arxiv.2101.03288}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Comparison with GANs}{10}{subsection.2.1.2}\protected@file@percent }
\citation{10.48550/arxiv.2101.03288}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Applications of NCE}{11}{subsection.2.1.3}\protected@file@percent }
\citation{10.48550/arxiv.2110.11271}
\citation{10.48550/arxiv.2110.11271}
\citation{10.48550/arxiv.2110.11271}
\citation{10.1561/2200000056}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Limitations of NCE}{12}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Variational Autoencoders (VAEs)}{12}{section.2.2}\protected@file@percent }
\citation{10.3390/jimaging4020036}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}VAE Architecture and Training}{13}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.1}VAE Structure}{13}{subsubsection.2.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces VAE structure showing the encoder and decoder processes.\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:VAE_structure}{{2.2}{13}{VAE structure showing the encoder and decoder processes.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.2}VAE Loss Function}{13}{subsubsection.2.2.1.2}\protected@file@percent }
\citation{10.1561/2200000056}
\citation{10.1088/2632-2153/ab80b7}
\citation{10.48550/arxiv.2002.10464}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Comparison with GANs}{14}{subsection.2.2.2}\protected@file@percent }
\citation{10.1109/access.2020.2977671}
\citation{10.48550/arxiv.1909.13062}
\citation{10.48550/arxiv.2106.06500}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Applications of VAEs}{15}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Limitations of VAEs}{15}{subsection.2.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Diffusion model structure showing the forward and reverse processes.\relax }}{16}{figure.caption.8}\protected@file@percent }
\newlabel{fig:Diffusion_structure}{{2.3}{16}{Diffusion model structure showing the forward and reverse processes.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Diffusion Models}{16}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Forward Process}{16}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Reverse Process}{17}{subsection.2.3.2}\protected@file@percent }
\citation{ho2020denoising}
\citation{nichol2021improved}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Loss Function}{18}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Comparison with GANs}{18}{subsection.2.3.4}\protected@file@percent }
\citation{chen2020wavegrad}
\citation{ho2020denoising}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Applications of Diffusion Models}{19}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Limitations of Diffusion Models}{19}{subsection.2.3.6}\protected@file@percent }
\citation{10.1007/s10928-021-09787-4}
\citation{10.1109/taslp.2017.2761547}
\citation{10.48550/arxiv.1802.05637}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Theoretical Background for GAN}{{3}{21}{Theoretical Background}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Objective Function of GAN}{21}{section.3.1}\protected@file@percent }
\newlabel{eq:min & max}{{3.1}{21}{Objective Function of GAN}{equation.3.1.1}{}}
\newlabel{eq:max}{{3.2}{22}{Objective Function of GAN}{equation.3.1.2}{}}
\newlabel{eq:min}{{3.3}{23}{Objective Function of GAN}{equation.3.1.3}{}}
\newlabel{eq:new min}{{3.4}{23}{Objective Function of GAN}{equation.3.1.4}{}}
\citation{10.1007/s10928-021-09787-4}
\citation{10.1109/taslp.2017.2761547}
\citation{10.1007/s10928-021-09787-4}
\citation{10.48550/arxiv.1802.05637}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of $\log (1 - D)$ and $-\log (D)$.\relax }}{24}{figure.caption.9}\protected@file@percent }
\newlabel{fig:log_function}{{3.1}{24}{Comparison of $\log (1 - D)$ and $-\log (D)$.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training Process of GAN}{24}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Distribution Changes During GAN Training}{25}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Diagram of GAN's training process. The green curve represents the distribution of generated samples. Initially, the generated samples may differ significantly from the real samples. As training progresses, the generated samples' distribution gradually approaches the real samples. The black dots represent the distribution of real samples, which remains unchanged throughout the training process and represents the target distribution. The blue dashed line represents the discriminator's output probability distribution. At the beginning of training, the discriminator can easily distinguish between real and generated data, resulting in a strong classification boundary. As training progresses and the generated data becomes more realistic, the discriminator's ability to differentiate between the two distributions weakens. Eventually, the discriminator's output approaches 0.5, indicating it can no longer effectively distinguish between real and generated data. The lines labeled $x$ and $z$ below represent the distribution of samples in the latent space. During GAN training, samples from the latent space $z$ are mapped to the data space $x$ through the generator. \relax }}{25}{figure.caption.10}\protected@file@percent }
\newlabel{fig:gan_training_process}{{3.2}{25}{Diagram of GAN's training process. The green curve represents the distribution of generated samples. Initially, the generated samples may differ significantly from the real samples. As training progresses, the generated samples' distribution gradually approaches the real samples. The black dots represent the distribution of real samples, which remains unchanged throughout the training process and represents the target distribution. The blue dashed line represents the discriminator's output probability distribution. At the beginning of training, the discriminator can easily distinguish between real and generated data, resulting in a strong classification boundary. As training progresses and the generated data becomes more realistic, the discriminator's ability to differentiate between the two distributions weakens. Eventually, the discriminator's output approaches 0.5, indicating it can no longer effectively distinguish between real and generated data. The lines labeled $x$ and $z$ below represent the distribution of samples in the latent space. During GAN training, samples from the latent space $z$ are mapped to the data space $x$ through the generator. \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Mathematical Formulation Changes During GAN Training}{26}{subsection.3.2.2}\protected@file@percent }
\newlabel{eq:derivative_to_zero}{{3.11}{27}{Mathematical Formulation Changes During GAN Training}{equation.3.2.11}{}}
\citation{10.48550/arxiv.2203.06026}
\citation{10.3390/app12157599}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Evaluating GAN Performance}{29}{section.3.3}\protected@file@percent }
\citation{10.1117/12.2673366}
\citation{10.1117/12.2673366}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Limitations of Accuracy in Evaluating GANs}{30}{section.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces GAN Training Accuracy Over Epochs\relax }}{30}{figure.caption.11}\protected@file@percent }
\newlabel{fig:my_picture}{{3.3}{30}{GAN Training Accuracy Over Epochs\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Generated Images from GAN\relax }}{31}{figure.caption.12}\protected@file@percent }
\newlabel{fig:my_picture}{{3.4}{31}{Generated Images from GAN\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{32}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Experiments}{{4}{32}{Results}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Standard GAN Versus Other GAN Realizations}{32}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}GAN With Convolutional or Dense layers}{33}{section.4.2}\protected@file@percent }
\newlabel{fig:gen_dense}{{4.1a}{34}{Generator with dense layer\relax }{figure.caption.13}{}}
\newlabel{sub@fig:gen_dense}{{a}{34}{Generator with dense layer\relax }{figure.caption.13}{}}
\newlabel{fig:gen_conv}{{4.1b}{34}{Generator with convolution layer\relax }{figure.caption.13}{}}
\newlabel{sub@fig:gen_conv}{{b}{34}{Generator with convolution layer\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Generator Architecture with Dense and Convolutional Layers\relax }}{34}{figure.caption.13}\protected@file@percent }
\newlabel{fig:gen_architecture}{{4.1}{34}{Generator Architecture with Dense and Convolutional Layers\relax }{figure.caption.13}{}}
\newlabel{fig:disc_dense}{{4.2a}{35}{Discriminator with dense layer\relax }{figure.caption.14}{}}
\newlabel{sub@fig:disc_dense}{{a}{35}{Discriminator with dense layer\relax }{figure.caption.14}{}}
\newlabel{fig:disc_conv}{{4.2b}{35}{Discriminator with convolution layer\relax }{figure.caption.14}{}}
\newlabel{sub@fig:disc_conv}{{b}{35}{Discriminator with convolution layer\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Discriminator Architecture with Dense and Convolutional Layers\relax }}{35}{figure.caption.14}\protected@file@percent }
\newlabel{fig:disc_architecture}{{4.2}{35}{Discriminator Architecture with Dense and Convolutional Layers\relax }{figure.caption.14}{}}
\newlabel{fig:generated_dense}{{4.3a}{35}{Images generated by GAN with dense layers\relax }{figure.caption.15}{}}
\newlabel{sub@fig:generated_dense}{{a}{35}{Images generated by GAN with dense layers\relax }{figure.caption.15}{}}
\newlabel{fig:generated_conv}{{4.3b}{35}{Images generated by GAN with convolution layers\relax }{figure.caption.15}{}}
\newlabel{sub@fig:generated_conv}{{b}{35}{Images generated by GAN with convolution layers\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Comparison of GAN Performance at 3000 Epochs\relax }}{35}{figure.caption.15}\protected@file@percent }
\newlabel{fig:generated_images}{{4.3}{35}{Comparison of GAN Performance at 3000 Epochs\relax }{figure.caption.15}{}}
\newlabel{fig:fid_dense}{{4.4a}{36}{FID score for dense layer\relax }{figure.caption.16}{}}
\newlabel{sub@fig:fid_dense}{{a}{36}{FID score for dense layer\relax }{figure.caption.16}{}}
\newlabel{fig:fid_conv}{{4.4b}{36}{FID score for convolutional layer\relax }{figure.caption.16}{}}
\newlabel{sub@fig:fid_conv}{{b}{36}{FID score for convolutional layer\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces FID Scores Across 3000 Epochs\relax }}{36}{figure.caption.16}\protected@file@percent }
\newlabel{fig:fid_scores}{{4.4}{36}{FID Scores Across 3000 Epochs\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Exploring Layer Depth}{37}{section.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Average FID Scores for Different GAN Architectures (Lower is Better)\relax }}{37}{table.caption.17}\protected@file@percent }
\newlabel{tab:fid_scores}{{4.1}{37}{Average FID Scores for Different GAN Architectures (Lower is Better)\relax }{table.caption.17}{}}
\citation{10.48550/arxiv.1703.10717}
\citation{10.48550/arxiv.2002.02112}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Impact of Data Augmentation on Model Performance}{38}{section.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Average FID Scores for Different Data Augmentation Techniques\relax }}{38}{table.caption.18}\protected@file@percent }
\newlabel{tab:augmented_training_average}{{4.2}{38}{Average FID Scores for Different Data Augmentation Techniques\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Applying the Model to the Animal Faces-HQ Dataset}{39}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Model Structure and Training}{39}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Generated Results}{42}{subsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Cat Faces Generated by GAN\relax }}{43}{figure.caption.19}\protected@file@percent }
\newlabel{fig:cat_faces_generated}{{4.5}{43}{Cat Faces Generated by GAN\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion}{44}{section.4.6}\protected@file@percent }
\newlabel{Discussion}{{4.6}{44}{Discussion}{section.4.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Code}{46}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Code}{{A}{46}{Code}{appendix.A}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}{\ignorespaces GAN model with dense layers}}{46}{lstlisting.A.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.2}{\ignorespaces GAN model with Convolutional layers}}{50}{lstlisting.A.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.3}{\ignorespaces Explore data augmentaion 1}}{54}{lstlisting.A.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.4}{\ignorespaces Explore data augmentation 2}}{59}{lstlisting.A.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.5}{\ignorespaces Explore data augmetation 3}}{63}{lstlisting.A.5}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.6}{\ignorespaces Explore GAN with more convolutional layers 1}}{68}{lstlisting.A.6}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.7}{\ignorespaces Explore GAN with more convolutional layers 2}}{72}{lstlisting.A.7}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.8}{\ignorespaces Explore GAN with more convolutional layers 3}}{76}{lstlisting.A.8}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.9}{\ignorespaces Explore GAN with more convolutional layers 4}}{81}{lstlisting.A.9}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.10}{\ignorespaces Explore GAN with more convolutional layers 5}}{86}{lstlisting.A.10}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.11}{\ignorespaces Explore GAN with more convolutional layers 6}}{91}{lstlisting.A.11}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.12}{\ignorespaces Apply new dataset}}{96}{lstlisting.A.12}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{References}
\bibcite{10.48550/arxiv.1704.00028}{{1}{}{{}}{{}}}
\bibcite{10.1002/mgea.30}{{2}{}{{}}{{}}}
\bibcite{10.1016/j.media.2019.101552}{{3}{}{{}}{{}}}
\bibcite{10.1016/j.artmed.2020.101938}{{4}{}{{}}{{}}}
\bibcite{10.1016/j.neucom.2018.09.013}{{5}{}{{}}{{}}}
\bibcite{10.1109/iccv.2019.00453}{{6}{}{{}}{{}}}
\bibcite{10.48550/arxiv.1711.00658}{{7}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2110.11271}{{8}{}{{}}{{}}}
\bibcite{10.18653/v1/e17-2003}{{9}{}{{}}{{}}}
\bibcite{10.21437/interspeech.2016-1295}{{10}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2101.03288}{{11}{}{{}}{{}}}
\bibcite{10.1561/2200000056}{{12}{}{{}}{{}}}
\bibcite{10.3390/jimaging4020036}{{13}{}{{}}{{}}}
\bibcite{10.1088/2632-2153/ab80b7}{{14}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2002.10464}{{15}{}{{}}{{}}}
\bibcite{10.1109/access.2020.2977671}{{16}{}{{}}{{}}}
\bibcite{10.48550/arxiv.1909.13062}{{17}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2106.06500}{{18}{}{{}}{{}}}
\bibcite{10.1007/s10928-021-09787-4}{{19}{}{{}}{{}}}
\bibcite{10.1109/taslp.2017.2761547}{{20}{}{{}}{{}}}
\bibcite{10.48550/arxiv.1802.05637}{{21}{}{{}}{{}}}
\bibcite{goodfellow2014generative}{{22}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2203.06026}{{23}{}{{}}{{}}}
\bibcite{10.3390/app12157599}{{24}{}{{}}{{}}}
\bibcite{10.1117/12.2673366}{{25}{}{{}}{{}}}
\bibcite{10.48550/arxiv.1703.10717}{{26}{}{{}}{{}}}
\bibcite{10.48550/arxiv.2002.02112}{{27}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{107}
