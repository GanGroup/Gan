\chapter{Historical Models of Image Generation}
\label{Related Work}

\section{Deep Boltzmann Machines}

Deep Boltzmann Machines (DBMs) are sophisticated generative models that excel in learning intricate data representations 
in an unsupervised manner. Unlike traditional feedforward networks, DBMs are structured with multiple layers of stochastic, 
binary latent variables, where each layer captures progressively abstract features of the input data. This architecture is 
characterized by fully connected layers, with no direct connections between units within the same layer, which enables DBMs 
to effectively model complex dependencies inherent in the data \citep{10.1007/978-3-642-40728-4_14}. The undirected nature 
of the connections in DBMs allows for the propagation of uncertainties across layers, enhancing their capability in feature 
learning and unsupervised pre-training \citep{10.1007/978-3-642-40728-4_14}\citep{10.48550/arxiv.1203.3783}.

However, the training of DBMs presents significant challenges, primarily due to their reliance on Markov Chain Monte Carlo (MCMC) methods, 
such as Gibbs sampling, for approximate inference. These methods are essential for estimating gradients and the intractable partition 
function, which is vital for updating model parameters. The computational overhead introduced by MCMC methods often results in 
slow convergence and increased complexity, particularly when applied to large-scale datasets \citep{10.48550/arxiv.2303.10728}. 
The requirement for numerous sampling steps to achieve equilibrium further complicates the training process, making DBMs less 
accessible compared to other generative models, such as Variational Autoencoders (VAEs).

To mitigate these training difficulties, DBMs often utilize layer-wise pre-training, which initializes model parameters 
before the entire network is fine-tuned. This approach has been shown to enhance the efficiency of the learning process
by providing a sensible initialization for the weights and variational inference \citep{10.1162/neco_a_00311}. Despite 
these strategies, the inherent complexity associated with MCMC methods continues to pose challenges in scaling DBMs 
effectively, particularly in comparison to more straightforward generative models like VAEs \citep{10.48550/arxiv.2303.10728}. 
Recent advancements have explored alternative training methodologies, including the use of specialized hardware systems to improve 
the efficiency of sampling tasks, thereby addressing some of the computational hurdles associated with traditional training approaches \citep{10.48550/arxiv.2303.10728}.



\section{Variational Autoencoders}
Variational Autoencoders (VAEs) are generative models in machine learning that excel in tasks like image generation and data 
representation by learning the underlying structure of data through a latent space. 

The architecture of VAEs consists of an encoder that maps input data into a latent space, where the latent variables are 
typically assumed to follow a Gaussian distribution. This assumption simplifies the learning process, as it allows for 
the use of techniques such as the reparameterization trick, which enables backpropagation through stochastic layers \citep{10.1561/2200000056}. 
The decoder then reconstructs the input data from the latent variables, ensuring that the model captures the essential 
features of the data distribution. The loss function of a VAE combines reconstruction loss with a Kullback-Leibler (KL) 
divergence term, which regularizes the latent space and encourages the model to learn a smooth and continuous representation \citep{10.3390/jimaging4020036}.

In comparison to DBMs, VAEs offer several advantages, including simpler training dynamics and better scalability. DBMs 
often require complex sampling methods, such as Gibbs sampling, which can be computationally intensive and slow to converge. 
In contrast, VAEs can be trained using standard gradient descent methods, making them more accessible for 
practitioners \citep{10.1561/2200000056}\citep{10.1109/access.2020.2977671}. Furthermore, the structured latent space of 
VAEs not only allows for efficient sampling but also supports various applications, such as image generation, anomaly detection, 
and data imputation, where the ability to interpolate between data points is crucial \citep{10.1088/2632-2153/ab80b7}\citep{10.48550/arxiv.2002.10464}.

However, VAEs face limitations in modeling discrete data as they require back-propagation through hidden units, which poses challenges 
in handling such data types effectively \citep{10.48550/arxiv.1909.13062}. Despite this limitation, VAEs have been extensively used to 
represent high-dimensional complex data by learning a low-dimensional latent space in an unsupervised manner \citep{10.48550/arxiv.2106.06500}.


\section{Noise Contrastive Estimation}


Noise-Contrastive Estimation (NCE) is a statistical method employed in machine learning, particularly for estimating parameters of unnormalized probabilistic models. The fundamental principle of NCE is to recast the maximum likelihood estimation problem into a binary classification task. In this framework, the model is trained to differentiate between actual data samples and artificially generated noise samples, which are drawn from a known distribution \citep{10.48550/arxiv.1711.00658}. This transformation simplifies the estimation process and circumvents the computational challenges associated with calculating the intractable partition function, a common hurdle in traditional maximum likelihood estimation methods for unnormalized models \citep{10.48550/arxiv.2110.11271}.

The methodology of NCE involves the introduction of noise samples, which serve as a baseline for comparison against real data. The model is trained to assign higher probabilities to genuine data samples while assigning lower probabilities to noise samples. This approach effectively enables the model to learn the underlying data distribution without the necessity of explicit normalization \citep{10.21437/interspeech.2016-1295}. The binary classification framework utilized in NCE allows for the application of standard logistic regression techniques, enhancing both the tractability and computational efficiency of the training process \citep{10.18653/v1/e17-2003}. Moreover, NCE has been shown to be particularly advantageous in large-scale models, such as energy-based models and word embeddings, where the computational burden of normalization can be substantial \citep{10.48550/arxiv.2101.03288}.

Despite its advantages, NCE is not without limitations. The choice of noise distribution is critical to the performance of the model; poorly selected noise distributions can lead to suboptimal parameter estimates and slow convergence rates \citep{10.48550/arxiv.2110.11271}. Additionally, while NCE simplifies the training of unnormalized models, it may not perform as effectively in scenarios where the noise distribution is challenging to define or when dealing with highly complex data distributions. The empirical observations regarding the importance of noise distribution have been formalized in various studies, underscoring the need for careful consideration in its selection \citep{10.48550/arxiv.2110.11271}.

