\chapter{Appendix}
\label{Code}
\section*{Code}

% First code block
\begin{lstlisting}[style=mypython, caption=GAN Model with Dense Layers]
import tensorflow as tf
from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from scipy.linalg import sqrtm
import numpy as np
import time
import matplotlib.pyplot as plt

np.random.seed(1000)
tf.random.set_seed(1000)

# input 100
# output 28*28*1

def build_generator():
    model = Sequential()
    
    # increase the dimension
    model.add(Dense(256, input_dim=100))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    
    model.add(Dense(28*28, activation='tanh'))
    model.add(Reshape((28, 28, 1)))

    return model

# input 28*28*1
# output 1

def build_discriminator():
    model = Sequential()
    
    model.add(Flatten(input_shape=(28, 28, 1)))
    
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    
    model.add(Dense(1, activation='sigmoid'))

    return model

# Load and preprocess the MNIST dataset
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
x_train = (x_train - 127.5) / 127.5
x_train = np.expand_dims(x_train, axis=3)

# Calculate FID function
def calculate_fid(real_images, fake_images):
    act1 = real_images.reshape((real_images.shape[0], -1))
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    
    act2 = fake_images.reshape((fake_images.shape[0], -1))
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    
    ssdiff = np.sum((mu1 - mu2)**2.0)
    covmean = sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    
    return fid

def train_gan(epochs=1000, batch_size=64, p_epoch=100):
    generator = build_generator()
    discriminator = build_discriminator()

    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    discriminator.trainable = False

    gan_input = tf.keras.Input(shape=(100,))
    gan_output = discriminator(generator(gan_input))
    gan = tf.keras.Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

    half_batch = int(batch_size / 2)
    
    d_losses = []
    g_losses = []
    d_acc = []
    fid_scores = []  # List to store FID scores
    
    start_time = time.time()  # Record the start time

    for epoch in range(epochs):
        # Select a random half batch of real images
        idx = np.random.randint(0, x_train.shape[0], half_batch)
        real_images = x_train[idx]

        # Generate a half batch of new fake images
        noise = np.random.normal(0, 1, (half_batch, 100))
        fake_images = generator.predict(noise)

        # Train the discriminator
        real_labels = np.ones((half_batch, 1))
        fake_labels = np.zeros((half_batch, 1))

        d_loss_real = discriminator.train_on_batch(real_images, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        valid_y = np.ones((batch_size, 1))

        g_loss = gan.train_on_batch(noise, valid_y)

        # Record the losses
        d_losses.append(d_loss[0])
        g_losses.append(g_loss)
        d_acc.append(d_loss[1] * 100)
        
        # Calculate and print FID every p_epoch epochs
        if epoch % p_epoch == 0:
            noise = np.random.normal(0, 1, (1000, 100))
            fake_images = generator.predict(noise)
            fid = calculate_fid(x_train[:1000], fake_images)
            fid_scores.append(fid)
            print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")

    end_time = time.time()  # Record the end time
    total_time = end_time - start_time
    print(f"Total training time: {total_time:.2f} seconds")
    return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}




\begin{lstlisting}[style=mypython, caption=GAN Model with Convolutional Layers]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        # increase the dimension
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    # Load and preprocess the MNIST dataset
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    # Calculate FID function
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        # using half batches for the discriminator ensures balanced and efficient training, 
        # better memory management, and more stable training dynamics in GANs.
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []  # List to store FID scores
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
        
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}


\begin{lstlisting}[style=mypython, caption= { Explore Data Augmentaion (rotation 10, width and height shift 0.1 \\ and  horizontal flip)}, captionpos=t]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        # increase the dimension
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True
    )
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            real_images_augmented = next(datagen.flow(real_images, batch_size=half_batch))
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images_augmented, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
    # Training the GAN with data augmentation and FID calculation
    generator, d_losses, g_losses, d_acc, fid_scores = train_gan(epochs=1000, batch_size=64, p_epoch=100)
\end{lstlisting}


\begin{lstlisting}[style=mypython, caption={Explore Data Augmentation (rotation {10}, width and \\ height shift {0.1})},captionpos=t]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        # increase the dimension
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
    )
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            real_images_augmented = next(datagen.flow(real_images, batch_size=half_batch))
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images_augmented, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
    # Training the GAN with data augmentation and FID calculation
    generator, d_losses, g_losses, d_acc, fid_scores = train_gan(epochs=1000, batch_size=64, p_epoch=100)
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption=Explore Data Augmetation (width and height shift {0.1})]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        # increase the dimension
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        width_shift_range=0.1,
        height_shift_range=0.1,
    )
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            real_images_augmented = next(datagen.flow(real_images, batch_size=half_batch))
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images_augmented, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
    # Training the GAN with data augmentation and FID calculation
    generator, d_losses, g_losses, d_acc, fid_scores = train_gan(epochs=1000, batch_size=64, p_epoch=100)
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption=Explore Data Augmetation (width and height shift {0.05})]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        # increase the dimension
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        width_shift_range=0.05,
        height_shift_range=0.05,
    )
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            real_images_augmented = next(datagen.flow(real_images, batch_size=half_batch))
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images_augmented, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
    # Training the GAN with data augmentation and FID calculation
    generator, d_losses, g_losses, d_acc, fid_scores = train_gan(epochs=1000, batch_size=64, p_epoch=100)
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 4 Convolutional Layers in Generator \\ and 3 Convolutional Layers in Discriminator}]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        # add 1 convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 5 Convolutional Layers in Generator \\ and 3 Convolutional Layers in Discriminator}]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the first convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 2nd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 6 Convolutional Layers in Generator \\ and 3 Convolutional Layers in Discriminator}]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 1st convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 2nd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 3rd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
    
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 6 Convolutional Layers in Generator \\ and 4 Convolutional Layers in Discriminator}]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 1st convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 2nd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 3rd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        # add the 1st convolution layer
        model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 6 Convolutional Layers in Generator \\ and 5 Convolutional Layers in Discriminator}]
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
    from scipy.linalg import sqrtm
    import numpy as np
    import time
    import matplotlib.pyplot as plt
    
    np.random.seed(1000)
    tf.random.set_seed(1000)
    
    # input 100
    # output 28*28*1
    
    def build_generator():
        model = Sequential()
        
        model.add(Dense(7*7*128, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((7, 7, 128)))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 1st convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 2nd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        # add the 3rd convolution layer
        model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
    
        model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))
    
        return model
    
    # input 28*28*1
    # output 1
    
    def build_discriminator():
        model = Sequential()
        
        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        # add the 1st convolution layer
        model.add(Conv2D(512, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        # add the 2nd convolution layer
        model.add(Conv2D(1024, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
    
        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))
    
        return model
    
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=3)
    
    def calculate_fid(real_images, fake_images):
        act1 = real_images.reshape((real_images.shape[0], -1))
        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
        
        act2 = fake_images.reshape((fake_images.shape[0], -1))
        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
        
        ssdiff = np.sum((mu1 - mu2)**2.0)
        covmean = sqrtm(sigma1.dot(sigma2))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
        
        return fid
    
    def train_gan(epochs=1000, batch_size=64, p_epoch=100):
        generator = build_generator()
        discriminator = build_discriminator()
    
        discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
        discriminator.trainable = False
    
        gan_input = tf.keras.Input(shape=(100,))
        gan_output = discriminator(generator(gan_input))
        gan = tf.keras.Model(gan_input, gan_output)
        gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
        half_batch = int(batch_size / 2)
        
        d_losses = []
        g_losses = []
        d_acc = []
        fid_scores = []
        
        start_time = time.time()  # Record the start time
    
        for epoch in range(epochs):
            # Select a random half batch of real images
            idx = np.random.randint(0, x_train.shape[0], half_batch)
            real_images = x_train[idx]
    
            # Generate a half batch of new fake images
            noise = np.random.normal(0, 1, (half_batch, 100))
            fake_images = generator.predict(noise)
    
            # Train the discriminator
            real_labels = np.ones((half_batch, 1))
            fake_labels = np.zeros((half_batch, 1))
    
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
            # Train the generator
            noise = np.random.normal(0, 1, (batch_size, 100))
            valid_y = np.ones((batch_size, 1))
    
            g_loss = gan.train_on_batch(noise, valid_y)
    
            # Record the losses
            d_losses.append(d_loss[0])
            g_losses.append(g_loss)
            d_acc.append(d_loss[1] * 100)
            
            # Calculate FID every p_epoch epochs
            if epoch % p_epoch == 0:
                noise = np.random.normal(0, 1, (1000, 100))
                fake_images = generator.predict(noise)
                fid = calculate_fid(x_train[:1000], fake_images)
                fid_scores.append(fid)
                print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")
    
        end_time = time.time()  # Record the end time
        total_time = end_time - start_time
        print(f"Total training time: {total_time:.2f} seconds")
        return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption= {Explore GAN with 6 Convolutional Layers in Generator \\ and 6 Convolutional Layers in Discriminator}]
    def build_generator():
    model = Sequential()
    
    model.add(Dense(7*7*128, input_dim=100))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Reshape((7, 7, 128)))
    model.add(BatchNormalization(momentum=0.8))

    # add the 1st convolution layer
    model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))

    # add the 2nd convolution layer
    model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))

    # add the 3rd convolution layer
    model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))

    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))

    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))

    model.add(Conv2D(1, kernel_size=7, activation='tanh', padding='same'))

    return model

def build_discriminator():
    model = Sequential()
    
    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

   # add the 1st convolution layer
    model.add(Conv2D(512, kernel_size=3, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    # add the 2nd convolution layer
    model.add(Conv2D(1024, kernel_size=3, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    # add the 3rd convolution layer
    model.add(Conv2D(2048, kernel_size=3, strides=2, padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))

    return model




import tensorflow as tf
from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from scipy.linalg import sqrtm
import numpy as np
import time
import matplotlib.pyplot as plt

np.random.seed(1000)
tf.random.set_seed(1000)

(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
x_train = (x_train - 127.5) / 127.5
x_train = np.expand_dims(x_train, axis=3)

def calculate_fid(real_images, fake_images):
    act1 = real_images.reshape((real_images.shape[0], -1))
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    
    act2 = fake_images.reshape((fake_images.shape[0], -1))
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    
    ssdiff = np.sum((mu1 - mu2)**2.0)
    covmean = sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    
    return fid

def train_gan(epochs=1000, batch_size=64, p_epoch=100):
    generator = build_generator()
    discriminator = build_discriminator()

    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    discriminator.trainable = False

    gan_input = tf.keras.Input(shape=(100,))
    gan_output = discriminator(generator(gan_input))
    gan = tf.keras.Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

    half_batch = int(batch_size / 2)
    
    d_losses = []
    g_losses = []
    d_acc = []
    fid_scores = []
    
    start_time = time.time()  # Record the start time

    for epoch in range(epochs):
        # Select a random half batch of real images
        idx = np.random.randint(0, x_train.shape[0], half_batch)
        real_images = x_train[idx]

        # Generate a half batch of new fake images
        noise = np.random.normal(0, 1, (half_batch, 100))
        fake_images = generator.predict(noise)

        # Train the discriminator
        real_labels = np.ones((half_batch, 1))
        fake_labels = np.zeros((half_batch, 1))

        d_loss_real = discriminator.train_on_batch(real_images, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        valid_y = np.ones((batch_size, 1))

        g_loss = gan.train_on_batch(noise, valid_y)

        # Record the losses
        d_losses.append(d_loss[0])
        g_losses.append(g_loss)
        d_acc.append(d_loss[1] * 100)
        
        # Calculate FID every p_epoch epochs
        if epoch % p_epoch == 0:
            noise = np.random.normal(0, 1, (1000, 100))
            fake_images = generator.predict(noise)
            fid = calculate_fid(x_train[:1000], fake_images)
            fid_scores.append(fid)
            print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}] [FID: {fid}]")

    end_time = time.time()  # Record the end time
    total_time = end_time - start_time
    print(f"Total training time: {total_time:.2f} seconds")
    return generator, d_losses, g_losses, d_acc, fid_scores
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption=Apply Animal Faces-HQ Dataset]

    import os
    import time
    import numpy as np
    import tensorflow as tf
    import matplotlib.pyplot as plt
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout, Input
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.preprocessing.image import load_img, img_to_array

    def load_images_as_rgb_matrices(path, size):
    images = []
    for img_name in os.listdir(path):
        img_path = os.path.join(path, img_name)
        img = load_img(img_path, target_size=size)
        img_array = img_to_array(img)
        images.append(img_array)
    images = np.array(images)
    images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]
    return images

    from google.colab import drive

    # mount Google Drive
    drive.mount('/content/drive')

    cat_path = "/content/drive/My Drive/gan/afhq/train/cat"
    size = (128, 128)
    x_train = load_images_as_rgb_matrices(cat_path, size)

    # generator
    # input 100
    # output 128*128*3

    def build_generator():
        model = Sequential()

        # Increase the dimension
        model.add(Dense(16*16*256, input_dim=100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Reshape((16, 16, 256)))
        model.add(BatchNormalization(momentum=0.8))

        model.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'))  # 32x32
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))

        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))  # 64x64
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))

        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))  # 128x128
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))

        model.add(Conv2D(3, kernel_size=7, activation='tanh', padding='same'))  # 128x128x3

    return model

    # discriminator
    # input 128*128*3
    # output 1

    def build_discriminator():
        model = Sequential()

        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=(128, 128, 3), padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))

        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))

        model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))

        model.add(Conv2D(512, kernel_size=3, strides=2, padding='same'))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))

        model.add(Flatten())
        model.add(Dense(1, activation='sigmoid'))

    return model

    # initial GAN
    def build_gan(generator, discriminator):
    discriminator.trainable = False
    gan_input = Input(shape=(100,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(0.0002, 0.5))
    return gan

    # define training step
    def train(generator, discriminator, gan, x_train, epochs, batch_size=128):
        valid = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))

        for epoch in range(epochs):
            start_time = time.time()


            idx = np.random.randint(0, x_train.shape[0], batch_size)
            imgs = x_train[idx]

            noise = np.random.normal(0, 1, (batch_size, 100))
            gen_imgs = generator.predict(noise)

            d_loss_real = discriminator.train_on_batch(imgs, valid)
            d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)


            noise = np.random.normal(0, 1, (batch_size, 100))
            g_loss = gan.train_on_batch(noise, valid)

            end_time = time.time()
            epoch_time = end_time - start_time

            print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}] [Epoch time: {epoch_time:.2f} seconds]")

    
    generator = build_generator()
    discriminator = build_discriminator()
    discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(0.0002, 0.5), metrics=['accuracy'])
    gan = build_gan(generator, discriminator)

    train(generator, discriminator, gan, x_train, epochs=4000, batch_size=64)

    # gen images
    def show_generated_images(generator, num_images=25, dim=(5, 5), figsize=(10, 10)):
        noise = np.random.normal(0, 1, (num_images, 100))
        gen_imgs = generator.predict(noise)
        gen_imgs = 0.5 * gen_imgs + 0.5

        plt.figure(figsize=figsize)
        for i in range(num_images):
            plt.subplot(dim[0], dim[1], i+1)
            plt.imshow(gen_imgs[i])
            plt.axis('off')
        plt.tight_layout()
        plt.show()

    show_generated_images(generator)
\end{lstlisting}